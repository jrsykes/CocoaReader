{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jrsykes/CocoaReader/blob/master/2step_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "YUFqzk2vnz4X",
        "outputId": "15c7ea34-3482-4df8-f445-b3c46a1ae336",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "doS-MIjpltcV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "!pip install torchmetrics\n",
        "from torchmetrics.classification import MulticlassPrecision, MulticlassRecall, MulticlassF1Score\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O4GVgqd1ltcX"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import os\n",
        "!pip install loguru\n",
        "from loguru import logger\n",
        "# import wandb\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.ssh\n",
        "!cp /content/drive/MyDrive/colab_ssh/id_ed25519 ~/.ssh/id_ed25519\n",
        "!chmod 600 ~/.ssh/id_ed25519\n",
        "\n",
        "# Add GitHub to known hosts\n",
        "!ssh-keyscan -t rsa github.com >> ~/.ssh/known_hosts\n"
      ],
      "metadata": {
        "id": "IUCuNMdutEfY",
        "outputId": "957dee5b-4d66-403e-c769-9a4313a97da4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# github.com:22 SSH-2.0-2e51c3195\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!GIT_SSH_COMMAND=\"ssh -i ~/.ssh/id_ed25519 -o IdentitiesOnly=yes\" git clone git@github.com:Fairfield-Vision/RT-DETR_cocoa.git\n"
      ],
      "metadata": {
        "id": "h3aPKyjEtkKG",
        "outputId": "389b0717-8f07-47a3-e9bc-0933b140bbd3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'RT-DETR_cocoa'...\n",
            "remote: Enumerating objects: 343, done.\u001b[K\n",
            "remote: Counting objects: 100% (20/20), done.\u001b[K\n",
            "remote: Compressing objects: 100% (13/13), done.\u001b[K\n",
            "remote: Total 343 (delta 7), reused 18 (delta 7), pack-reused 323 (from 1)\u001b[K\n",
            "Receiving objects: 100% (343/343), 139.71 MiB | 28.70 MiB/s, done.\n",
            "Resolving deltas: 100% (44/44), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/RT-DETR_cocoa')\n",
        "\n",
        "from HiveSight_utils import create_rtdetr_model, load_partial_weights, NPYDataset\n",
        "from HiveSight_utils import export_to_onnx, draw_bboxes_on_image, custom_collate_fn\n"
      ],
      "metadata": {
        "id": "JZK-B6YJtr7R"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "LtRXAyEMltcY"
      },
      "outputs": [],
      "source": [
        "from rtdetrv2_pytorch.src.zoo.rtdetr.rtdetr_criterion import RTDETRCriterion\n",
        "from rtdetrv2_pytorch.src.zoo.rtdetr.matcher import HungarianMatcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "tWESIdGlltcY",
        "outputId": "e6b38301-dae8-4080-f985-c05e755bacd2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "logger.add(\"train_script.log\", rotation=\"500 MB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "25aTAiDNltcZ"
      },
      "outputs": [],
      "source": [
        "def freeze_layers(model, freeze_backbone=True):\n",
        "    \"\"\"Freeze/unfreeze specific layers in the model.\"\"\"\n",
        "    for param in model.backbone.parameters():\n",
        "        param.requires_grad = not freeze_backbone\n",
        "    for param in model.encoder.parameters():\n",
        "        param.requires_grad = freeze_backbone\n",
        "    for param in model.decoder.parameters():\n",
        "        param.requires_grad = freeze_backbone\n",
        "    for param in model.regression_head.parameters():\n",
        "        param.requires_grad = not freeze_backbone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "BP-ZXpwlltcZ"
      },
      "outputs": [],
      "source": [
        "def filter_bbox_samples(dataset):\n",
        "    \"\"\"Filter dataset to include only samples with bounding box annotations.\"\"\"\n",
        "    return [sample for sample in dataset if len(sample[2]) > 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pp_1JyGxltcZ"
      },
      "outputs": [],
      "source": [
        "def train(config=None):\n",
        "    image_root = \"/users/jrs596/scratch/EC25/data_test\"\n",
        "    n_classes = len(os.listdir(image_root))\n",
        "    annotation_root = \"/users/jrs596/longship/yolo_annotations\"\n",
        "    RT_DETR_weights = \"/users/jrs596/scratch/TORCH_HOME/hub/checkpoints/rtdetrv2_r18vd_120e_coco_rerun_48.1.pth\"\n",
        "    batch_size = 100\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    weight_dict = {\n",
        "        \"cost_class\": config.cost_class_weight,\n",
        "        \"cost_bbox\": config.cost_bbox_weight,\n",
        "        \"cost_giou\": config.cost_giou_weight\n",
        "    }\n",
        "    matcher = HungarianMatcher(weight_dict=weight_dict, use_focal_loss=False)\n",
        "    rtdetr_criterion = RTDETRCriterion(\n",
        "        matcher=matcher,\n",
        "        weight_dict={\n",
        "            \"loss_ce\": config.loss_ce_weight,\n",
        "            \"loss_bbox\": config.loss_bbox_weight,\n",
        "            \"loss_giou\": config.loss_giou_weight\n",
        "        },\n",
        "        losses=['labels', 'boxes', 'cardinality'],\n",
        "        alpha=0.25,\n",
        "        gamma=2.0,\n",
        "        eos_coef=0.1,\n",
        "        num_classes=1\n",
        "    )\n",
        "    rtdetr_criterion.to(device)\n",
        "    classification_criterion = torch.nn.CrossEntropyLoss()\n",
        "    precision = MulticlassPrecision(num_classes=n_classes, average='macro').to(device)\n",
        "    recall = MulticlassRecall(num_classes=n_classes, average='macro').to(device)\n",
        "    f1_score = MulticlassF1Score(num_classes=n_classes, average='macro').to(device)\n",
        "    logger.info(\"Creating datasets and data loaders\")\n",
        "    dataset = NPYDataset(image_root, annotation_root)\n",
        "    train_size = int(0.8 * len(dataset))\n",
        "    val_size = len(dataset) - train_size\n",
        "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=custom_collate_fn)\n",
        "    model = create_rtdetr_model(num_classes_bb=2, num_queries=300, n_classes=n_classes)\n",
        "    checkpoint = torch.load(RT_DETR_weights, map_location=\"cpu\", weights_only=True)\n",
        "    state_dict = checkpoint['ema']['module']\n",
        "    load_partial_weights(model.encoder, state_dict, \"encoder\")\n",
        "    load_partial_weights(model.decoder, state_dict, \"decoder\")\n",
        "    model.to(device)\n",
        "    best_f1_score, best_giou = 0.0, float(\"inf\")\n",
        "\n",
        "    # Phase 1: Train Backbone & Classification Head\n",
        "    logger.info(\"Phase 1: Training classification head with frozen transformer\")\n",
        "    freeze_layers(model, freeze_backbone=False)\n",
        "    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=config.learning_rate)\n",
        "    phase_1_running = True\n",
        "\n",
        "    while phase_1_running:\n",
        "        model.train()\n",
        "        for images, labels, _ in tqdm(train_loader, desc=\"Phase 1 Training\"):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            _, _, pred_labels = model(images)\n",
        "            loss = classification_criterion(pred_labels, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        f1 = f1_score.compute().item()\n",
        "        if f1 <= best_f1_score:\n",
        "            phase_1_running = False\n",
        "        else:\n",
        "            best_f1_score = f1\n",
        "        f1_score.reset()\n",
        "\n",
        "    # Phase 2: Train Transformer for Bounding Box Detection (only samples with bounding boxes)\n",
        "    train_dataset = filter_bbox_samples(train_dataset)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn)\n",
        "\n",
        "    logger.info(\"Phase 2: Training transformer with frozen classification head\")\n",
        "    freeze_layers(model, freeze_backbone=True)\n",
        "    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=config.learning_rate)\n",
        "    phase_2_running = True\n",
        "\n",
        "    while phase_2_running:\n",
        "        model.train()\n",
        "        for images, _, bboxes in tqdm(train_loader, desc=\"Phase 2 Training\"):\n",
        "            images = images.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            pred_logits, pred_boxes, _ = model(images)\n",
        "            loss_dict = rtdetr_criterion({\"pred_logits\": pred_logits, \"pred_boxes\": pred_boxes}, bboxes)\n",
        "            loss = sum(loss_dict.values())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        giou = loss_dict[\"loss_giou\"].item()\n",
        "        if giou >= best_giou:\n",
        "            phase_2_running = False\n",
        "        else:\n",
        "            best_giou = giou\n",
        "\n",
        "#     wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egbZEex8ltca"
      },
      "source": [
        "if __name__ == \"__main__\":<br>\n",
        "    wandb.init(project=\"HiveSight-RT-DETR_cocoa\", config={<br>\n",
        "        \"learning_rate\": 1e-4,<br>\n",
        "        \"cost_class_weight\": 1.0,<br>\n",
        "        \"cost_bbox_weight\": 5.0,<br>\n",
        "        \"cost_giou_weight\": 2.0,<br>\n",
        "        \"loss_ce_weight\": 1.0,<br>\n",
        "        \"loss_bbox_weight\": 5.0,<br>\n",
        "        \"loss_giou_weight\": 2.0<br>\n",
        "    })<br>\n",
        "    train(config=wandb.config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5RwMmcHnltcb"
      },
      "outputs": [],
      "source": [
        "config={\n",
        "        \"learning_rate\": 1e-4,\n",
        "        \"cost_class_weight\": 1.0,\n",
        "        \"cost_bbox_weight\": 5.0,\n",
        "        \"cost_giou_weight\": 2.0,\n",
        "        \"loss_ce_weight\": 1.0,\n",
        "        \"loss_bbox_weight\": 5.0,\n",
        "        \"loss_giou_weight\": 2.0\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gjB8KYSultcb"
      },
      "outputs": [],
      "source": [
        "train(config=config)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}